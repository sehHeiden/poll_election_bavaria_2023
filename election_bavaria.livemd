# Bavarian Election

```elixir
Mix.install(
  [
    {:bumblebee, "~> 0.3"},
    {:explorer, "~> 0.7"},
    {:kino_vega_lite, "~> 0.1.7"},
    {:httpoison, "~> 1.8"},
    {:kino_bumblebee, "~> 0.3.0"},
    {:exla, "~> 0.5.1"},
    {:axon_onnx, "~> 0.4"},
    {:kino_db, "~> 0.2.1"},
    {:adbc, "~> 0.1"}
  ],
  config: [
    nx: [default_backend: EXLA.Backend]
  ]
)
```

## Polls

```elixir
require Explorer.DataFrame

alias VegaLite, as: Vl
alias Explorer.DataFrame, as: DF
alias Explorer.Series, as: S
```

Load the csv with the polls from different instiutions form the website [wahlrecht.de](https://www.wahlrecht.de/umfragen/landtage/bayern.htm#fn-bp).
Show the timeline and trend for every party.

```elixir
polls = DF.from_csv!("polls.csv")
```

```elixir
defmodule Graphs do
  def create_graph(data_source, title, party) do
    Vl.new(width: 500, height: 300, title: title)
    |> Vl.data_from_values(data_source, only: ["end_date", party, "institute"])
    |> Vl.layers([
      Vl.new()
      |> Vl.mark(:point)
      |> Vl.encode_field(:x, "end_date", type: :temporal, title: "poll end date")
      |> Vl.encode_field(:y, party, type: :quantitative, title: "percentage")
      |> Vl.encode_field(:color, "institute", type: :nominal),
      Vl.new()
      |> Vl.mark(:line, color: "firebrick", opacity: 0.5)
      |> Vl.transform(regression: party, on: "end_date")
      |> Vl.encode_field(:x, "end_date", type: :temporal, title: "poll end date")
      |> Vl.encode_field(:y, party, type: :quantitative, title: "percentage")
    ])
  end
end
```

```elixir
Graphs.create_graph(polls, "Polls - CSU", "csu")
```

```elixir
Graphs.create_graph(polls, "Polls - Buendnis-Gruene", "gruene")
```

```elixir
Graphs.create_graph(polls, "Polls - Freie Waehler", "fw")
```

```elixir
Graphs.create_graph(polls, "Polls - AFD", "afd")
```

```elixir
Graphs.create_graph(polls, "Polls - SPD", "spd")
```

```elixir
Graphs.create_graph(polls, "Polls - FDP", "fdp")
```

```elixir
Graphs.create_graph(polls, "Polls - Linke", "linke")
```

## Parties

```elixir
parties_df =
  DF.new(
    party: ["csu", "fw", "spd", "gruene", "fdp", "afd", "linke"],
    candiate1: [
      "Markus Söder",
      "Hubert Aiwanger",
      "Florian von Brunn",
      "Ludwig Hartmann",
      "Martin Hagen",
      "Katrin Ebner-Steiner",
      "Adelheid Rupp"
    ],
    candidate2: [nil, nil, nil, "Katharina Schulze", nil, "Martin Böhm", nil]
  )

DF.head(parties_df)
```

## Preprocessing posts for sentiment analysis

Before the sentiments can be read from the posts. It is nessescary to remove html tags.
The mastodon tags, need to be converted to text.

```elixir
# f = "./mastodon/mastodon_bayernwahl2023.db"
f = "mastodon_bayernwahl2023_20230910.db"
p = Path.absname(f)
```

<!-- livebook:{"reevaluate_automatically":true} -->

```elixir
Adbc.download_driver!(:sqlite)
{:ok, db} = Kino.start_child({Adbc.Database, driver: :sqlite, uri: p})

{:ok, conn} = Kino.start_child({Adbc.Connection, database: db})
```

<!-- livebook:{"reevaluate_automatically":true} -->

```elixir
{:ok, toots_df} = Explorer.DataFrame.from_query(conn, "select * from toots", [])
```

Remove:

* HTML
* #-Sign
* @-Sign
* _-Sign
* links

Left in:

* Simileys (language model might know them)
* numbers (language model converts them)

<!-- livebook:{"reevaluate_automatically":true} -->

```elixir
posts =
  toots_df[:content]
  |> Explorer.Series.to_list()

r =
  Regex.compile!(
    "<[^>]*>|#|@|_|https?:\/\/(?:www\.)?([-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b)*(\/[\/\d\w\.-]*)*(?:[\?])*(.+)*|="
  )

cleared_posts = Enum.map(posts, &Regex.replace(r, &1, ""))
```

## Classification

Before the sentiment analysis. The langauage has to be checked. As the language attribute is very often not correct.
Therefore a language detection has to be made first with **XLM-RoBERTa - language detection**.
Than the german text are evaluated with **german-sentiment_bert**.

English texts with **RoBERTa (BERTtweet) - Sentiment**.

### Language Detection

```elixir
{:ok, model_info} = Bumblebee.load_model({:hf, "papluca/xlm-roberta-base-language-detection"})

{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "papluca/xlm-roberta-base-language-detection"})

serving =
  Bumblebee.Text.text_classification(model_info, tokenizer,
    compile: [batch_size: length(cleared_posts), sequence_length: 100],
    defn_options: [compiler: EXLA]
  )
```

```elixir
p = Nx.Serving.run(serving, cleared_posts)
```

Each predictions is ordered by probybility. Hence always selecting the label the first language returns the most likely one.

```elixir
detected_languages =
  Enum.map(p, fn post ->
    post
    |> Enum.at(0)
    |> elem(1)
    |> Enum.at(0)
    |> (& &1[:label]).()
  end)
```

The majority of 96% of all saved posts are detected as german. 1.3 % are detected as English.
Why 1 %  are detected as Thai and 0.5 % are detected as Hindi has to be find out. That 0.6 % are detected as Dutch is more plausable.

```elixir
detected_languages
|> Enum.frequencies()
|> Enum.sort_by(&elem(&1, 1), :desc)
|> Enum.map(fn {lang, freq} -> {lang, freq / length(detected_languages) * 100.0} end)
```

In contrast the manually set language are 92%, 6 % language (often the default) and 1.3 % nil (not specified).

```elixir
toots_df["language"]
|> S.to_list()
|> Enum.frequencies()
|> Enum.sort_by(&elem(&1, 1), :desc)
|> Enum.map(fn {lang, freq} -> {lang, freq / length(detected_languages) * 100.0} end)
```

```elixir
toots_df = DF.put(toots_df, "detected_languages", detected_languages)
toots_df = DF.put(toots_df, "cleared_content", cleared_posts)
```

From visual analysis the langauge attribute is often set wrong, as it set manually, with a given default. Often the language was set to English, when it was German or set to nil. Therefore the language has been evaluated by language detection model, which changed the language in 8.4 % of all posts.

```elixir
reasigned_language =
  toots_df["language"]
  |> S.not_equal(toots_df["detected_languages"])
  |> S.to_list()
  |> Enum.map(&if &1, do: 1, else: 0)
  |> Enum.sum()

reasigned_language / S.size(toots_df["language"]) * 100.0
```

ToDo:

1. Batched run -> less ram, longer texts

<!-- livebook:{"break_markdown":true} -->

### German Sentiments

```elixir
{:ok, model_info} = Bumblebee.load_model({:hf, "oliverguhr/german-sentiment-bert"})
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "bert-base-german-cased"})
```

```elixir
german_toots_df = DF.filter(toots_df, detected_languages == "de")
german_toots = S.to_list(german_toots_df["cleared_content"])
```

```elixir
serving =
  Bumblebee.Text.text_classification(model_info, tokenizer, defn_options: [compiler: EXLA])

Nx.Serving.run(serving, german_toots)
```

ToDo:

1. batched run of sentiment analysis
2. combine the scores as for English
3. same todos as for English

<!-- livebook:{"break_markdown":true} -->

### Englisch Sentiment

```elixir
english_toots_df = DF.filter(toots_df, detected_languages == "en")
english_toots = S.to_list(english_toots_df["cleared_content"])
```

```elixir
defmodule SentimentScore do
  def score(prediction) do
    prediction
    |> Enum.map(fn p ->
      cond do
        p.label == "POS" -> p.score
        p.label == "NEG" -> -p.score
        p.label == "NEU" -> 0
      end
    end)
    |> Enum.sum()
  end
end
```

```elixir
{:ok, model_info} = Bumblebee.load_model({:hf, "finiteautomata/bertweet-base-sentiment-analysis"})
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "vinai/bertweet-base"})

serving =
  Bumblebee.Text.text_classification(model_info, tokenizer,
    compile: [batch_size: length(english_toots), sequence_length: 100],
    defn_options: [compiler: EXLA]
  )
```

```elixir
predictions = Nx.Serving.run(serving, english_toots)
sentiments = Enum.map(predictions, fn x -> SentimentScore.score(x.predictions) end)
DF.put(english_toots_df, "sentiment", sentiments)
```

ToDo:

1. Match the sentiments to a party.
2. Date -> calendar weeks

## Attribution

ToDo:

1. Filter: Topic of posts is really about Bavaria
2. Attribution of Sentiment to a single party
3. Filter: No party, multiple parties
