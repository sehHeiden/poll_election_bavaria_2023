# Bavarian Election

```elixir
Mix.install(
  [
    {:nx, "~>0.6.1"},
    {:bumblebee, github: :"elixir-nx/bumblebee"},
    {:explorer,
     git: "https://github.com/elixir-explorer/explorer.git",
     ref: "aef274989ab490b0a392ccd19ec24b286a8cda1c",
     override: true},
    {:kino_vega_lite, "~> 0.1.10"},
    {:httpoison, "~> 1.8"},
    {:exla, "~> 0.6.1"},
    {:adbc, "~> 0.1"}
  ],
  config: [
    nx: [default_backend: EXLA.Backend]
  ]
)
```

## Polls

```elixir
require Explorer.DataFrame

alias VegaLite, as: Vl
alias Explorer.DataFrame, as: DF
alias Explorer.Series, as: S
```

```elixir
start_date = ~N[2023-08-29 00:00:01]
```

Load the csv with the polls from different instiutions form the website [wahlrecht.de](https://www.wahlrecht.de/umfragen/landtage/bayern.htm#fn-bp).
Show the timeline and trend for every party.

```elixir
polls = DF.from_csv!("polls.csv")

polls_dt =
  S.to_list(polls["end_date"])
  |> Enum.map(&elem(NaiveDateTime.from_iso8601(&1 <> " 00:00:01"), 1))
  |> S.from_list()

polls = DF.put(polls, "end_date", polls_dt)
```

```elixir
defmodule PollGraphs do
  def create_graph(data_source, title, party, start_date) do
    Vl.new(width: 500, height: 300, title: title)
    |> Vl.data_from_values(DF.filter(data_source, end_date > ^start_date),
      only: ["end_date", party, "institute"]
    )
    |> Vl.layers([
      Vl.new()
      |> Vl.mark(:point)
      |> Vl.encode_field(:x, "end_date", type: :temporal, title: "poll end date")
      |> Vl.encode_field(:y, party, type: :quantitative, title: "percentage")
      |> Vl.encode_field(:color, "institute", type: :nominal),
      Vl.new()
      |> Vl.mark(:line, color: "firebrick", opacity: 0.5)
      |> Vl.transform(loess: party, on: "end_date", bandwidth: 0.8)
      |> Vl.encode_field(:x, "end_date", type: :temporal, title: "poll end date")
      |> Vl.encode_field(:y, party, type: :quantitative, title: "percentage")
    ])
  end

  def create_graph(data_source, title, party) do
    create_graph(data_source, title, party, ~N[2023-01-01 00:00:01])
  end
end
```

```elixir
PollGraphs.create_graph(polls, "Polls - CSU", "csu")
```

```elixir
PollGraphs.create_graph(polls, "Polls - Freie Waehler", "fw")
```

```elixir
PollGraphs.create_graph(polls, "Polls - Buendnis90-Gruene", "gruene")
```

```elixir
PollGraphs.create_graph(polls, "Polls - SPD", "spd")
```

```elixir
PollGraphs.create_graph(polls, "Polls - FDP", "fdp")
```

```elixir
PollGraphs.create_graph(polls, "Polls - AFD", "afd")
```

```elixir
PollGraphs.create_graph(polls, "Polls - Linke", "linke")
```

## Parties

```elixir
parties_df =
  DF.new(
    party: ["csu", "fw", "spd", "gruene", "fdp", "afd", "linke"],
    candiate1: [
      "Markus Söder",
      "Hubert Aiwanger",
      "Florian von Brunn",
      "Ludwig Hartmann",
      "Martin Hagen",
      "Katrin Ebner-Steiner",
      "Adelheid Rupp"
    ],
    candidate2: [nil, nil, nil, "Katharina Schulze", nil, "Martin Böhm", nil]
  )

DF.print(parties_df)
```

```elixir
parties = S.to_list(parties_df["party"])

parties_regex =
  parties
  |> Enum.join("|")
  |> Regex.compile!()
```

## Analysis Tags and Toots

```elixir
# f = "./mastodon/mastodon_bayernwahl2023.db"
f = "mastodon_bayernwahl2023_20230910.db"
p = Path.absname(f)
```

<!-- livebook:{"reevaluate_automatically":true} -->

```elixir
Adbc.download_driver!(:sqlite)
{:ok, db} = Kino.start_child({Adbc.Database, driver: :sqlite, uri: p})

{:ok, conn} = Kino.start_child({Adbc.Connection, database: db})
```

```elixir
{:ok, tags_df} = Explorer.DataFrame.from_query(conn, "select * from tags", [])

tags_df["tag"]
|> S.downcase()
|> S.to_list()
|> Enum.filter(&Enum.member?(parties, &1))
|> Enum.frequencies()
```

From the original tracked posts, only a minimal set contains the names of the parties:

* Freie Waehler (82)
* Gruene (58)
* Linke (57)

This is much better for the parties. AFD, CSU and SPD.

<!-- livebook:{"reevaluate_automatically":true} -->

```elixir
{:ok, toots_df} = Explorer.DataFrame.from_query(conn, "select * from toots", [])
```

```elixir
post_number = S.size(toots_df["content"])
```

```elixir
toots_df["content"]
|> S.downcase()
|> S.to_list()
|> Enum.filter(&Regex.match?(parties_regex, &1))
|> Enum.map(&Regex.scan(parties_regex, &1))
|> Enum.map(&Enum.uniq(&1))
|> List.flatten()
|> Enum.frequencies()
|> Enum.sort_by(&elem(&1, 1), :desc)
|> Enum.map(fn {lang, freq} -> {lang, freq / post_number * 100.0} end)
```

This is slightly getting better, the whole posts are taken into account.

* Freie Waehler (82 -> 155)
* Grune (58 -> 169)
* Linke (57 -> 175)

```elixir
defmodule Names do
  def append_variants(names) do
    names
    |> Enum.concat(Enum.map(names, fn x -> String.replace(x, " ", "") end))
    |> Enum.concat(Enum.map(names, fn x -> String.replace(x, "ö", "o") end))
    |> Enum.concat(Enum.map(names, fn x -> String.replace(x, "ö", "oe") end))
    |> Enum.concat(Enum.map(names, fn x -> String.replace(x, "-", "") end))
    |> Enum.uniq()
  end

  def family_name(y) do
    String.split(y, " ", parts: 2) |> Enum.at(1)
  end
end

candidate_family_names =
  parties_df["candiate1"]
  |> S.concat(parties_df["candidate2"])
  |> S.downcase()
  |> S.to_list()
  |> Enum.filter(&is_binary(&1))
  |> Enum.map(&Names.family_name(&1))

candidate_family_names =
  candidate_family_names
  |> Names.append_variants()
  |> Enum.join("|")

bavaria_tags = Regex.compile!("bayern|csu|" <> candidate_family_names)
```

```elixir
name_regex = Regex.compile!(candidate_family_names)
```

## Preprocessing posts for sentiment analysis

Before the sentiments can be read from the posts. It is nessescary to remove html tags.
The mastodon tags, need to be converted to text.

<!-- livebook:{"break_markdown":true} -->

Convert date column from string to naive date time.

```elixir
dt =
  S.to_list(toots_df["date"])
  |> Enum.map(&elem(NaiveDateTime.from_iso8601(&1), 1))
  |> S.from_list()

toots_df = DF.put(toots_df, "date", dt)
```

Remove:

* HTML
* #-Sign
* @-Sign
* _-Sign
* links

Left in:

* Simileys (language model might know them)
* numbers (language model converts them)

<!-- livebook:{"reevaluate_automatically":true} -->

```elixir
posts =
  toots_df[:content]
  |> Explorer.Series.to_list()

r =
  Regex.compile!(
    "<[^>]*>|#|@|_|https?:\/\/(?:www\.)?([-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b)*(\/[\/\d\w\.-]*)*(?:[\?])*(.+)*|="
  )

cleared_posts = Enum.map(posts, &Regex.replace(r, &1, ""))
```

```elixir
toots_df = DF.put(toots_df, "cleared_content", cleared_posts)
```

```elixir
post_s =
  cleared_posts
  |> Enum.map(&String.length(&1))
  |> Enum.filter(&(&1 > 0))
  |> S.from_list()

post_s
|> S.mean()
```

```elixir
post_s
|> S.standard_deviation()
```

```elixir
post_s
|> S.median()
```

On the mode of the cleared posts is 205 characters in the first set (After removing the length of zero). The mean is 235+/190 character.

## Filter posts

ToDo:

1. Filter: Topic of posts is really about Bavaria
2. Attribution of Sentiment to a single party
3. Filter: No party, multiple parties

<!-- livebook:{"break_markdown":true} -->

Filter: Contains at least a single cantidate.

```elixir
bavarian_post_filter =
  toots_df["cleared_content"]
  |> S.downcase()
  |> S.transform(&Regex.match?(bavaria_tags, &1))

bavarian_toots_df = DF.mask(toots_df, bavarian_post_filter)
```

Filter all posts that the toots, contains only a single party or its candidates per post.

```elixir
party_regexes =
  DF.to_rows(parties_df)
  # only per line in DataFrame
  |> Enum.map(fn x ->
    x
    # only takes the values
    |> Map.values()
    # remove nils
    |> Enum.filter(&is_bitstring(&1))
    # take family names if candidate anmes
    |> Enum.map(fn y ->
      cond do
        Names.family_name(y) == nil -> y
        true -> Names.family_name(y)
      end
    end)
    |> Names.append_variants()
    |> Enum.join("|")
    |> String.downcase()
    |> Regex.compile!()
  end)
```

```elixir
defmodule Bool do
  def to_integer(true), do: 1
  def to_integer(false), do: 0
  def to_integer(nil), do: 0
end

contains_party =
  Enum.map(party_regexes, fn reg ->
    bavarian_toots_df["cleared_content"]
    |> S.downcase()
    |> S.to_list()
    |> Enum.map(&Regex.match?(reg, &1))
  end)

parties_count =
  contains_party
  |> Enum.map(fn x -> Enum.map(x, &Bool.to_integer(&1)) end)
  |> Nx.tensor()
  |> Nx.sum(axes: [0])

contains_single_parties =
  parties_count
  |> Nx.to_list()
  |> Enum.map(&(&1 == 1))
```

```elixir
parties_count
|> Nx.to_list()
|> Enum.frequencies()
```

Most posts name a single party. Very often two parties are mentioned in a single post. The other cases are much less frequent.

```elixir
mentioned_parties =
  Enum.zip(contains_party, S.to_list(parties_df["party"]))
  |> Enum.map(fn x -> Enum.map(elem(x, 0), &if(&1, do: elem(x, 1), else: "")) end)
  |> Enum.zip()
  |> Enum.map(&Tuple.to_list(&1))
  |> Enum.map(&Enum.join(&1, ""))

bavarian_toots_df = DF.put(bavarian_toots_df, "mentioned_party", mentioned_parties)

single_party_toots_df = DF.mask(bavarian_toots_df, contains_single_parties)
```

## Classification

Before the sentiment analysis. The langauage has to be checked. As the language attribute is very often not correct.
Therefore a language detection has to be made first, before the sentiment analysis

1. **XLM-RoBERTa** - language detection
2. **german-sentiment_bert** - Sentiment Analysis German
3. **RoBERTa (BERTtweet) - Sentiment** - English language Sentiment analysis

### Language Detection

The modules takes a different number of maximum characters. The language detection takes up to 514 characters, but the results updates of the self set languages is similar to 100 characters. Therefore we restrict to 100 characters.

<!-- livebook:{"break_markdown":true} -->

<!-- Learn more at https://mermaid-js.github.io/mermaid -->

```mermaid
flowchart TD;
  A(XLM-RoBERTa) -->|German| B[german-sentiment_bert];
  A(XLM-RoBERTa) -->|English| C[RoBERTa BERTtweet - Sentiment];

```

```elixir
{:ok, lang_detect_model_info} =
  Bumblebee.load_model({:hf, "papluca/xlm-roberta-base-language-detection"})

{:ok, lang_detect_tokenizer} =
  Bumblebee.load_tokenizer({:hf, "papluca/xlm-roberta-base-language-detection"})

lang_detect_serving =
  Bumblebee.Text.text_classification(lang_detect_model_info, lang_detect_tokenizer,
    compile: [batch_size: 256, sequence_length: 100],
    defn_options: [compiler: EXLA]
  )

lang_detect_model_info.spec.max_positions
```

```elixir
Kino.start_child({
  Nx.Serving,
  serving: lang_detect_serving, name: LangDetectServer
})
```

```elixir
p = Nx.Serving.batched_run(LangDetectServer, S.to_list(single_party_toots_df["cleared_content"]))
```

Each predictions is ordered by probybility. Hence always selecting the label the first language returns the most likely one.

```elixir
detected_languages =
  Enum.map(p, fn post ->
    post
    |> Enum.at(0)
    |> elem(1)
    |> Enum.at(0)
    |> (& &1[:label]).()
  end)
```

The majority of 96% of all saved posts are detected as german. 1.3 % are detected as English.
Why 1 %  are detected as Thai and 0.5 % are detected as Hindi has to be find out. That 0.6 % are detected as Dutch is more plausable.

```elixir
detected_languages
|> Enum.frequencies()
|> Enum.sort_by(&elem(&1, 1), :desc)
|> Enum.map(fn {lang, freq} -> {lang, freq / length(detected_languages) * 100.0} end)
```

In contrast the manually set language are 92%, 6 % language (often the default) and 1.3 % nil (not specified).

```elixir
single_party_toots_df["language"]
|> S.to_list()
|> Enum.frequencies()
|> Enum.sort_by(&elem(&1, 1), :desc)
|> Enum.map(fn {lang, freq} -> {lang, freq / length(detected_languages) * 100.0} end)
```

```elixir
single_party_toots_df = DF.put(single_party_toots_df, "detected_languages", detected_languages)
```

From visual analysis the langauge attribute is often set wrong, as it set manually, with a given default. Often the language was set to English, when it was German or set to nil. Therefore the language has been evaluated by language detection model, which changed the language in 8.4 % of all posts.

```elixir
reasigned_language =
  single_party_toots_df["language"]
  |> S.not_equal(single_party_toots_df["detected_languages"])
  |> S.to_list()
  |> Enum.map(&Bool.to_integer(&1))
  |> Enum.sum()

reasigned_language / S.size(toots_df["language"]) * 100.0
```

### German Sentiments

<!-- livebook:{"break_markdown":true} -->

The German sentiment analysis works with up to 512 characters. Per standard Mastodon set the limit to 500 characters. We cut the string length of the strings to µ + s, at 425 character.

```elixir
{:ok, ger_sent_model_info} = Bumblebee.load_model({:hf, "oliverguhr/german-sentiment-bert"})
{:ok, ger_sent_tokenizer} = Bumblebee.load_tokenizer({:hf, "bert-base-german-cased"})
ger_sent_model_info.spec.max_positions
```

```elixir
ger_sent_serving =
  Bumblebee.Text.text_classification(ger_sent_model_info, ger_sent_tokenizer,
    compile: [batch_size: 256, sequence_length: 42],
    defn_options: [compiler: EXLA]
  )

Kino.start_child({
  Nx.Serving,
  serving: ger_sent_serving, name: GerSentimentServer
})
```

```elixir
german_toots_df = DF.filter(single_party_toots_df, detected_languages == "de")
german_toots = S.to_list(german_toots_df["cleared_content"])
```

```elixir
ger_predictions = Nx.Serving.batched_run(GerSentimentServer, german_toots)
```

```elixir
Enum.at(ger_predictions, 0).predictions
```

```elixir
defmodule SentimentScore do
  def score(prediction) do
    prediction
    |> Enum.map(fn p ->
      case p do
        %{label: l} when l in ["POS", "positive"] -> p.score
        %{label: l} when l in ["NEG", "negative"] -> -p.score
        _ -> 0
      end
    end)
    |> Enum.sum()
  end
end
```

```elixir
ger_sentiments = Enum.map(ger_predictions, fn x -> SentimentScore.score(x.predictions) end)
german_toots_df = DF.put(german_toots_df, "sentiment", ger_sentiments)
```

### Englisch Sentiment

The sentiment analysis on the English text works only with 130 tokens.

```elixir
english_toots_df = DF.filter(single_party_toots_df, detected_languages == "en")
english_toots = S.to_list(english_toots_df["cleared_content"])
```

```elixir
{:ok, model_info} = Bumblebee.load_model({:hf, "finiteautomata/bertweet-base-sentiment-analysis"})
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "vinai/bertweet-base"})

english_sentiment_serving =
  Bumblebee.Text.text_classification(model_info, tokenizer,
    compile: [batch_size: 256, sequence_length: 130],
    defn_options: [compiler: EXLA]
  )
```

```elixir
Kino.start_child({
  Nx.Serving,
  serving: english_sentiment_serving, name: EngSentimentServer
})
```

```elixir
eng_predictions = Nx.Serving.batched_run(EngSentimentServer, english_toots)
```

```elixir
eng_sentiments = Enum.map(eng_predictions, fn x -> SentimentScore.score(x.predictions) end)
english_toots_df = DF.put(english_toots_df, "sentiment", eng_sentiments)
```

```elixir
ger_eng_toots = DF.concat_rows(german_toots_df, english_toots_df)
```

```elixir
DF.filter(ger_eng_toots, date > ^start_date)[:mentioned_party]
|> S.to_list()
|> Enum.frequencies()
```

ToDo:

1. Match the sentiments to a party.
2. Date -> calendar weeks

```elixir
defmodule SentimentGraphs do
  def create_graph(data_source, title, party, start_date) do
    Vl.new(width: 500, height: 300, title: title)
    |> Vl.data_from_values(
      DF.filter(data_source, mentioned_party == ^party and date > ^start_date),
      only: ["date", "sentiment", "detected_languages"]
    )
    |> Vl.layers([
      Vl.new()
      |> Vl.mark(:point)
      |> Vl.encode_field(:x, "date", type: :temporal)
      |> Vl.encode_field(:y, "sentiment", type: :quantitative)
      |> Vl.encode_field(:color, "detected_languages", type: :nominal, title: "Detected Languages"),
      Vl.new()
      |> Vl.mark(:line, color: "firebrick", opacity: 0.5)
      |> Vl.transform(loess: "sentiment", on: "date", bandwidth: 0.8)
      |> Vl.encode_field(:x, "date", type: :temporal, title: "date")
      |> Vl.encode_field(:y, "sentiment", type: :quantitative, title: "sentiment")
    ])
  end
end
```

```elixir
SentimentGraphs.create_graph(ger_eng_toots, "Sentiments for the CSU", "csu", start_date)
```

```elixir
SentimentGraphs.create_graph(ger_eng_toots, "Sentiments for the Freie Waehler", "fw", start_date)
```

```elixir
SentimentGraphs.create_graph(
  ger_eng_toots,
  "Sentiments for the Buendnis90/Gruene",
  "gruene",
  start_date
)
```

```elixir
SentimentGraphs.create_graph(ger_eng_toots, "Sentiments for the SPD", "spd", start_date)
```

```elixir
SentimentGraphs.create_graph(ger_eng_toots, "Sentiments for the FDP", "fdp", start_date)
```

```elixir
SentimentGraphs.create_graph(ger_eng_toots, "Sentiments for the AFD", "afd", start_date)
```

```elixir
SentimentGraphs.create_graph(ger_eng_toots, "Sentiments for the Linke", "linke", start_date)
```

## Attribution

* gender

* bavarian

* age (perhaps)

* sentiment -> Party

* date -> calendar week

```elixir
{:ok, person_df} = Explorer.DataFrame.from_query(conn, "select * from users", [])
```

```elixir
bavarian_instances = ~w"muenchen.social augsburg.social mastodon.bayern nuernberg.social 
ploen.social wue.social mastodon.dachgau.social  sueden.social"
```
